#!/usr/bin/env python3
"""
è®­ç»ƒç»„ä»¶
æä¾›RT-DETRè®­ç»ƒåŠŸèƒ½ï¼Œå‚è€ƒultimate_sanity_check.pyçš„éªŒè¯å®ç°

æ”¯æŒEMA (Exponential Moving Average) è®­ç»ƒ
"""

import os
import sys
import time
import pickle
import numpy as np

import jittor as jt

# å¯¼å…¥EMAæ¨¡å—
try:
    from src.optim.ema import ModelEMA, create_ema
    EMA_AVAILABLE = True
except ImportError:
    EMA_AVAILABLE = False
    ModelEMA = None
    create_ema = None


class RTDETRTrainer:
    """
    RT-DETRè®­ç»ƒå™¨
    å‚è€ƒultimate_sanity_check.pyçš„éªŒè¯å®ç°
    """
    def __init__(self, model, criterion, optimizer, save_dir="./results",
                 use_ema=False, ema_decay=0.9999, ema_warmups=2000):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.save_dir = save_dir

        # EMAæ”¯æŒ
        self.ema = None
        if use_ema:
            if EMA_AVAILABLE:
                self.ema = ModelEMA(model, decay=ema_decay, warmups=ema_warmups)
                print(f"EMA enabled: decay={ema_decay}, warmups={ema_warmups}")
            else:
                print("Warning: EMA requested but module not available")

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

        # è®­ç»ƒå†å²
        self.train_losses = []
        self.epoch_times = []

        print(f"Training results will be saved to: {save_dir}")
    
    def train_epoch(self, dataset, epoch, total_epochs):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        å‚è€ƒultimate_sanity_check.pyçš„å®ç°
        """
        self.model.train()
        epoch_losses = []
        epoch_start_time = time.time()
        
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = np.random.permutation(len(dataset))
        
        for i, idx in enumerate(indices):
            # åŠ è½½æ•°æ®
            images, targets = dataset[idx]
            
            # æ·»åŠ batchç»´åº¦
            images = images.unsqueeze(0)
            targets = [targets]
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(images, targets)
            
            # æŸå¤±è®¡ç®—
            loss_dict = self.criterion(outputs, targets)
            total_loss = sum(loss_dict.values())
            
            # åå‘ä¼ æ’­ - ä½¿ç”¨éªŒè¯è¿‡çš„æ–¹æ³•
            self.optimizer.step(total_loss)

            # å…³é”®: å¼ºåˆ¶æ‰§è¡Œè®¡ç®—å›¾ï¼Œé˜²æ­¢Jittoræƒ°æ€§æ±‚å€¼å¯¼è‡´çš„å†…å­˜æ³„æ¼
            jt.sync_all()

            # EMAæ›´æ–° - åœ¨ä¼˜åŒ–å™¨æ­¥éª¤åæ›´æ–°
            if self.ema is not None:
                self.ema.update(self.model)

            epoch_losses.append(float(total_loss.item()))
            
            # æ‰“å°è¿›åº¦ï¼ˆæ¯10%æˆ–å‰å‡ ä¸ªbatchï¼‰
            if i < 5 or (i + 1) % max(1, len(indices) // 10) == 0:
                print(f"     Batch {i+1}/{len(indices)}: æŸå¤± = {total_loss.item():.4f}")
        
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)
        
        # è®°å½•å†å²
        self.train_losses.append(avg_loss)
        self.epoch_times.append(epoch_time)
        
        print(f"   Epoch {epoch+1:3d}/{total_epochs}: å¹³å‡æŸå¤± = {avg_loss:.4f}, ç”¨æ—¶ = {epoch_time:.1f}s")
        
        return avg_loss, epoch_time
    
    def train(self, dataset, num_epochs, lr_decay_epochs=None, lr_decay_factor=0.5):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            dataset: è®­ç»ƒæ•°æ®é›†
            num_epochs: è®­ç»ƒè½®æ•°
            lr_decay_epochs: å­¦ä¹ ç‡è¡°å‡çš„epochåˆ—è¡¨
            lr_decay_factor: å­¦ä¹ ç‡è¡°å‡å› å­
        """
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {len(dataset)} å¼ å›¾åƒï¼Œ{num_epochs} è½®")
        print("=" * 60)
        
        start_time = time.time()
        
        for epoch in range(num_epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            avg_loss, epoch_time = self.train_epoch(dataset, epoch, num_epochs)
            
            # å­¦ä¹ ç‡è¡°å‡
            if lr_decay_epochs and (epoch + 1) in lr_decay_epochs:
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] *= lr_decay_factor
                print(f"   å­¦ä¹ ç‡è¡°å‡åˆ°: {self.optimizer.param_groups[0]['lr']:.6f}")
        
        total_time = time.time() - start_time
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   åˆå§‹æŸå¤±: {self.train_losses[0]:.4f}")
        print(f"   æœ€ç»ˆæŸå¤±: {self.train_losses[-1]:.4f}")
        print(f"   æŸå¤±ä¸‹é™: {self.train_losses[0] - self.train_losses[-1]:.4f}")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’")
        print(f"   å¹³å‡æ¯è½®æ—¶é—´: {total_time/num_epochs:.1f}ç§’")
        
        return {
            'train_losses': self.train_losses,
            'epoch_times': self.epoch_times,
            'total_time': total_time,
            'num_epochs': num_epochs,
            'final_loss': self.train_losses[-1],
            'loss_reduction': self.train_losses[0] - self.train_losses[-1]
        }
    
    def save_model(self, filename="rtdetr_model.pkl"):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.save_dir, filename)
        self.model.save(model_path)
        print(f"Model saved to: {model_path}")

        # å¦‚æœæœ‰EMAï¼Œä¹Ÿä¿å­˜EMAæ¨¡å‹
        if self.ema is not None:
            ema_filename = filename.replace('.pkl', '_ema.pkl')
            ema_path = os.path.join(self.save_dir, ema_filename)
            self.ema.module.save(ema_path)
            print(f"EMA model saved to: {ema_path}")

        return model_path

    def save_checkpoint(self, filename="checkpoint.pkl"):
        """ä¿å­˜å®Œæ•´çš„æ£€æŸ¥ç‚¹ï¼ˆåŒ…æ‹¬æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€EMAçŠ¶æ€ï¼‰"""
        checkpoint = {
            'model': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'epoch_times': self.epoch_times,
        }
        if self.ema is not None:
            checkpoint['ema'] = self.ema.state_dict()

        checkpoint_path = os.path.join(self.save_dir, filename)
        jt.save(checkpoint, checkpoint_path)
        print(f"Checkpoint saved to: {checkpoint_path}")
        return checkpoint_path

    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = jt.load(checkpoint_path)
        self.model.load_state_dict(checkpoint['model'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])

        if 'train_losses' in checkpoint:
            self.train_losses = checkpoint['train_losses']
        if 'epoch_times' in checkpoint:
            self.epoch_times = checkpoint['epoch_times']

        if self.ema is not None and 'ema' in checkpoint:
            self.ema.load_state_dict(checkpoint['ema'])
            print('Loading ema.state_dict')

        print(f"Checkpoint loaded from: {checkpoint_path}")

    def get_eval_model(self):
        """
        è·å–ç”¨äºè¯„ä¼°çš„æ¨¡å‹

        å¦‚æœå¯ç”¨äº†EMAï¼Œè¿”å›EMAæ¨¡å‹ï¼›å¦åˆ™è¿”å›åŸå§‹æ¨¡å‹
        """
        if self.ema is not None:
            return self.ema.module
        return self.model
    
    def save_training_results(self, results, filename="training_results.pkl"):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        results_path = os.path.join(self.save_dir, filename)
        with open(results_path, 'wb') as f:
            pickle.dump(results, f)
        print(f"ğŸ“Š è®­ç»ƒç»“æœä¿å­˜åˆ°: {results_path}")
        return results_path
    
    def get_training_stats(self):
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.train_losses:
            return None
        
        return {
            'total_epochs': len(self.train_losses),
            'initial_loss': self.train_losses[0],
            'final_loss': self.train_losses[-1],
            'best_loss': min(self.train_losses),
            'loss_reduction': self.train_losses[0] - self.train_losses[-1],
            'loss_reduction_percent': (self.train_losses[0] - self.train_losses[-1]) / self.train_losses[0] * 100,
            'total_time': sum(self.epoch_times),
            'avg_epoch_time': np.mean(self.epoch_times),
            'convergence_rate': (self.train_losses[0] - self.train_losses[-1]) / sum(self.epoch_times)
        }
    
    def print_training_summary(self):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        stats = self.get_training_stats()
        if stats is None:
            print("âš ï¸ æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return
        
        print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“:")
        print(f"   è®­ç»ƒè½®æ•°: {stats['total_epochs']}")
        print(f"   åˆå§‹æŸå¤±: {stats['initial_loss']:.4f}")
        print(f"   æœ€ç»ˆæŸå¤±: {stats['final_loss']:.4f}")
        print(f"   æœ€ä½³æŸå¤±: {stats['best_loss']:.4f}")
        print(f"   æŸå¤±ä¸‹é™: {stats['loss_reduction']:.4f} ({stats['loss_reduction_percent']:.1f}%)")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {stats['total_time']:.1f}ç§’")
        print(f"   å¹³å‡æ¯è½®æ—¶é—´: {stats['avg_epoch_time']:.1f}ç§’")
        print(f"   æ”¶æ•›é€Ÿåº¦: {stats['convergence_rate']:.6f} æŸå¤±/ç§’")

def create_trainer(model, criterion, optimizer, save_dir="./results",
                   use_ema=False, ema_decay=0.9999, ema_warmups=2000):
    """
    åˆ›å»ºè®­ç»ƒå™¨çš„å·¥å‚å‡½æ•°

    Args:
        model: RT-DETRæ¨¡å‹
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        save_dir: ä¿å­˜ç›®å½•
        use_ema: æ˜¯å¦ä½¿ç”¨EMA
        ema_decay: EMAè¡°å‡ç‡
        ema_warmups: EMAé¢„çƒ­æ­¥æ•°

    Returns:
        trainer: RT-DETRè®­ç»ƒå™¨
    """
    trainer = RTDETRTrainer(
        model, criterion, optimizer, save_dir,
        use_ema=use_ema, ema_decay=ema_decay, ema_warmups=ema_warmups
    )
    return trainer


def quick_train(model, criterion, optimizer, dataset, num_epochs=50, save_dir="./results",
                use_ema=True, ema_decay=0.9999, ema_warmups=2000):
    """
    å¿«é€Ÿè®­ç»ƒå‡½æ•°

    Args:
        model: RT-DETRæ¨¡å‹
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        dataset: è®­ç»ƒæ•°æ®é›†
        num_epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•
        use_ema: æ˜¯å¦ä½¿ç”¨EMA (é»˜è®¤True)
        ema_decay: EMAè¡°å‡ç‡
        ema_warmups: EMAé¢„çƒ­æ­¥æ•°

    Returns:
        trainer: è®­ç»ƒå™¨
        results: è®­ç»ƒç»“æœ
    """
    # åˆ›å»ºè®­ç»ƒå™¨ (é»˜è®¤å¯ç”¨EMA)
    trainer = create_trainer(
        model, criterion, optimizer, save_dir,
        use_ema=use_ema, ema_decay=ema_decay, ema_warmups=ema_warmups
    )

    # å¼€å§‹è®­ç»ƒ
    results = trainer.train(dataset, num_epochs, lr_decay_epochs=[30, 40], lr_decay_factor=0.5)

    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    trainer.save_model("rtdetr_trained.pkl")
    trainer.save_training_results(results)

    # æ‰“å°æ€»ç»“
    trainer.print_training_summary()

    return trainer, results

if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒç»„ä»¶
    print("ğŸ§ª æµ‹è¯•RT-DETRè®­ç»ƒç»„ä»¶")
    print("=" * 50)
    
    try:
        print("âš ï¸ è®­ç»ƒç»„ä»¶éœ€è¦é…åˆæ¨¡å‹å’Œæ•°æ®é›†ç»„ä»¶ä½¿ç”¨")
        print("   è¯·å‚è€ƒtrain_script.pyä¸­çš„å®Œæ•´ä½¿ç”¨ç¤ºä¾‹")
        
        print(f"\nğŸ‰ RT-DETRè®­ç»ƒç»„ä»¶éªŒè¯å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
